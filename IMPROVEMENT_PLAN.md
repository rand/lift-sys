# System Improvement Plan
**Version**: 0.2.0 â†’ 0.2.1 â†’ 0.3.0
**Date**: October 11, 2025
**Based On**: Comprehensive Test Report

---

## Overview

This plan addresses critical bugs found in testing and outlines improvements for reaching production readiness.

**Current State:** 83.8% tests passing, CLI/SDK broken due to auth issues
**Target State:** 100% tests passing, all interfaces functional, production-ready

---

## Phase 1: Critical Hotfixes (v0.2.1) - **Priority: IMMEDIATE**
**Timeline:** 1-2 days
**Goal:** Make CLI and SDK usable

### 1.1 Fix CLI Authentication Support

**Issue:** CLI-001 - CLI doesn't pass demo user header
**Severity:** ðŸ”´ CRITICAL
**Estimated Time:** 2 hours

**Changes Required:**

**File:** `lift_sys/cli/session_commands.py`

```python
def get_client(api_url: str = "http://localhost:8000") -> SessionClient:
    """Get configured session client."""
    # Add header support
    headers = {}

    # Check for demo user mode
    if os.getenv("LIFT_SYS_ENABLE_DEMO_USER_HEADER") == "1":
        # Use demo user header
        demo_user = os.getenv("LIFT_SYS_DEMO_USER", "cli-user")
        headers["x-demo-user"] = demo_user

    return SessionClient(base_url=api_url, headers=headers)
```

**Testing:**
```bash
# Test CLI with demo mode
LIFT_SYS_ENABLE_DEMO_USER_HEADER=1 \
  uv run python -m lift_sys.cli session create --prompt "Test"
```

**Success Criteria:**
- âœ… CLI commands work with `LIFT_SYS_ENABLE_DEMO_USER_HEADER=1`
- âœ… CLI provides clear error when auth is missing
- âœ… All CLI integration tests pass

### 1.2 Fix SDK Authentication Support

**Issue:** SDK-001 - SDK doesn't auto-detect demo mode
**Severity:** ðŸ”´ CRITICAL
**Estimated Time:** 1 hour

**Changes Required:**

**File:** `lift_sys/client/session_client.py`

```python
class SessionClient:
    """Client for interacting with lift-sys session management API."""

    def __init__(
        self,
        base_url: str = "http://localhost:8000",
        headers: Optional[Dict[str, str]] = None,
        timeout: float = 30.0,
    ):
        self.base_url = base_url.rstrip("/")
        self.timeout = timeout

        # Initialize headers with defaults
        self.headers = headers or {}

        # Auto-detect demo user mode
        if not self.headers.get("x-demo-user"):
            if os.getenv("LIFT_SYS_ENABLE_DEMO_USER_HEADER") == "1":
                demo_user = os.getenv("LIFT_SYS_DEMO_USER", "sdk-user")
                self.headers["x-demo-user"] = demo_user
```

**Testing:**
```python
import os
os.environ["LIFT_SYS_ENABLE_DEMO_USER_HEADER"] = "1"

from lift_sys.client import SessionClient
client = SessionClient()  # Should auto-detect demo mode
session = client.create_session(prompt="Test")
assert session.session_id
```

**Success Criteria:**
- âœ… SDK auto-detects `LIFT_SYS_ENABLE_DEMO_USER_HEADER`
- âœ… SDK still allows manual header override
- âœ… Example script works without modification
- âœ… All SDK integration tests pass

### 1.3 Fix CLI Module Warning

**Issue:** CLI-002 - RuntimeWarning about module in sys.modules
**Severity:** ðŸŸ¡ HIGH
**Estimated Time:** 1 hour

**Changes Required:**

**File:** `lift_sys/cli/__main__.py`

```python
"""CLI entry point for lift-sys."""
from __future__ import annotations

def main() -> None:
    """Main entry point for CLI."""
    # Import locally to avoid circular import issues
    import typer
    from .session_commands import app as session_app

    # Create main CLI app
    cli_app = typer.Typer(
        name="lift-sys",
        help="Lift-sys command line interface",
        no_args_is_help=True,
    )

    # Add session commands
    cli_app.add_typer(session_app, name="session")

    # Run
    cli_app()


if __name__ == "__main__":
    main()
```

**File:** `lift_sys/cli/__init__.py`

```python
"""CLI commands for lift-sys."""
from __future__ import annotations

__all__ = ["main"]

def main() -> None:
    """Entry point for CLI."""
    from .__main__ import main as cli_main
    cli_main()
```

**Testing:**
```bash
# Should not show RuntimeWarning
uv run python -m lift_sys.cli --help 2>&1 | grep -i warning
```

**Success Criteria:**
- âœ… No RuntimeWarning appears
- âœ… CLI still functions correctly
- âœ… Help text displays properly

### 1.4 Update Deprecated datetime.utcnow()

**Issue:** API-001 - Deprecation warnings
**Severity:** âš ï¸ MEDIUM
**Estimated Time:** 1 hour

**Changes Required:**

**Files to update:**
- `lift_sys/api/server.py` (3 locations)
- `tests/conftest.py` (1 location)

**Find and replace:**
```python
# OLD
datetime.utcnow()

# NEW
datetime.now(datetime.UTC)
```

**Testing:**
```bash
# Should have 0 datetime warnings
uv run pytest tests/ -W error::DeprecationWarning 2>&1 | grep datetime
```

**Success Criteria:**
- âœ… No `datetime.utcnow()` deprecation warnings
- âœ… All tests still pass
- âœ… Timestamps remain in UTC

### 1.5 Update FastAPI Lifespan

**Issue:** API-001 - FastAPI on_event deprecation
**Severity:** âš ï¸ MEDIUM
**Estimated Time:** 30 minutes

**Changes Required:**

**File:** `lift_sys/api/server.py`

```python
from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan manager."""
    # Startup
    logger.info("Application starting up")
    yield
    # Shutdown
    logger.info("Application shutting down")

app = FastAPI(
    title="lift-sys API",
    lifespan=lifespan  # Use lifespan instead of on_event
)

# Remove these:
# @app.on_event("startup")
# @app.on_event("shutdown")
```

**Testing:**
```bash
# Should have 0 on_event warnings
uv run pytest tests/ -W error::DeprecationWarning 2>&1 | grep on_event
```

**Success Criteria:**
- âœ… No `on_event` deprecation warnings
- âœ… Server starts and stops correctly
- âœ… All tests still pass

---

## Phase 2: Fix Integration Tests (v0.2.1) - **Priority: HIGH**
**Timeline:** 1-2 days
**Goal:** Achieve 100% test pass rate

### 2.1 Fix API Integration Test

**Issue:** TEST-001 - `test_repos_open_endpoint` fails
**Severity:** ðŸŸ¡ HIGH
**Estimated Time:** 1 hour

**Investigation Required:**
```python
# Check what 400 error is returned
def test_repos_open_endpoint_debug(api_client):
    response = api_client.post(
        "/repos/open",
        json={"identifier": "octocat/example"},
    )
    print(f"Status: {response.status_code}")
    print(f"Body: {response.json()}")
    # Debug why it returns 400
```

**Testing:**
```bash
uv run pytest tests/integration/test_api_endpoints.py::TestAPIEndpoints::test_repos_open_endpoint -v
```

**Success Criteria:**
- âœ… Test returns 200 OK
- âœ… Root cause identified and fixed

### 2.2 Fix CLI Integration Tests

**Issue:** TEST-001 - 24 CLI integration tests fail
**Severity:** ðŸŸ¡ HIGH
**Estimated Time:** 2 hours (should be fixed by 1.1)

**Changes Required:**
After fixing CLI auth (1.1), update test fixtures:

**File:** `tests/integration/test_cli_session_commands.py`

```python
import os
import pytest

@pytest.fixture(autouse=True)
def enable_demo_mode():
    """Enable demo mode for all CLI tests."""
    os.environ["LIFT_SYS_ENABLE_DEMO_USER_HEADER"] = "1"
    os.environ["LIFT_SYS_DEMO_USER"] = "test-cli"
    yield
    # Clean up
    os.environ.pop("LIFT_SYS_ENABLE_DEMO_USER_HEADER", None)
    os.environ.pop("LIFT_SYS_DEMO_USER", None)
```

**Testing:**
```bash
LIFT_SYS_ENABLE_DEMO_USER_HEADER=1 \
  uv run pytest tests/integration/test_cli_session_commands.py -v
```

**Success Criteria:**
- âœ… All 24 CLI integration tests pass
- âœ… Tests don't require running server (use api_client fixture)

### 2.3 Fix or Remove TUI Integration Tests

**Issue:** TUI-001 - 16/19 TUI tests fail
**Severity:** âš ï¸ MEDIUM
**Estimated Time:** 3 hours

**Option A: Fix Tests (Preferred)**

Update tests to match actual TUI structure:

**File:** `tests/integration/test_tui_session_management.py`

```python
# Remove tests that don't match actual implementation
# Keep only tests that verify:
# - TUI has session_client attribute
# - TUI has required methods
# - Methods don't crash when called

# Remove detailed integration tests that require complex mocking
```

**Option B: Move to E2E Suite**

Move TUI tests to `tests/e2e/` and mark as requiring `textual.testing`:

```python
pytest.mark.skipif(
    not textual_testing_available,
    reason="Requires textual.testing"
)
```

**Testing:**
```bash
uv run pytest tests/integration/test_tui_session_management.py -v
```

**Success Criteria:**
- âœ… All remaining TUI tests pass
- âœ… Tests accurately reflect TUI capabilities

---

## Phase 3: Documentation Updates (v0.2.1) - **Priority: MEDIUM**
**Timeline:** 1 day
**Goal:** Clarify auth requirements

### 3.1 Update README Quick Start

**Issue:** DOC-001 - Auth requirements not clear
**Severity:** ðŸŸ¡ MEDIUM
**Estimated Time:** 30 minutes

**Changes Required:**

**File:** `README.md`

Add prominent auth section:

```markdown
## Quick Start: Session Management

âš ï¸ **Authentication Required**: All interfaces require authentication.
For development, use demo mode:

\```bash
export LIFT_SYS_ENABLE_DEMO_USER_HEADER=1
\```

### Web UI
\```bash
export LIFT_SYS_ENABLE_DEMO_USER_HEADER=1
./start.sh
# Navigate to http://localhost:5173
\```

### CLI
\```bash
export LIFT_SYS_ENABLE_DEMO_USER_HEADER=1
uv run python -m lift_sys.cli session create --prompt "Test"
\```

### Python SDK
\```python
import os
os.environ["LIFT_SYS_ENABLE_DEMO_USER_HEADER"] = "1"

from lift_sys.client import SessionClient
client = SessionClient()  # Auto-detects demo mode
\```
```

**Success Criteria:**
- âœ… Auth requirements are prominent
- âœ… Demo mode is clearly explained
- âœ… All interface examples include auth setup

### 3.2 Update Workflow Guides

**File:** `docs/WORKFLOW_GUIDES.md`

Add auth section to each workflow:

```markdown
### Prerequisites
- Backend running on port 8000
- **Demo mode enabled**: `export LIFT_SYS_ENABLE_DEMO_USER_HEADER=1`
```

**Success Criteria:**
- âœ… Every workflow mentions auth
- âœ… Troubleshooting section includes auth issues

### 3.3 Update FAQ

**File:** `docs/FAQ.md`

Add auth questions:

```markdown
### Why do I get "Connection refused" errors?

**Most common cause:** Demo mode not enabled.

**Solution:**
\```bash
export LIFT_SYS_ENABLE_DEMO_USER_HEADER=1
\```

### Why do I get 401 Unauthorized?

**Cause:** API requires authentication, but no credentials provided.

**Solutions:**
1. Enable demo mode: `export LIFT_SYS_ENABLE_DEMO_USER_HEADER=1`
2. Configure OAuth (for production)
```

**Success Criteria:**
- âœ… Auth issues are in FAQ
- âœ… Solutions are clear and actionable

---

## Phase 4: Improve Error Messages (v0.2.1) - **Priority: MEDIUM**
**Timeline:** 1 day
**Goal:** Better debugging experience

### 4.1 Add Auth Error Detection in CLI

**File:** `lift_sys/cli/session_commands.py`

```python
def get_client(api_url: str = "http://localhost:8000") -> SessionClient:
    """Get configured session client with auth checking."""
    headers = {}

    # Check for demo user mode
    if os.getenv("LIFT_SYS_ENABLE_DEMO_USER_HEADER") == "1":
        demo_user = os.getenv("LIFT_SYS_DEMO_USER", "cli-user")
        headers["x-demo-user"] = demo_user
    else:
        # Warn user about auth requirements
        console.print("[yellow]âš ï¸  Demo mode not enabled[/yellow]")
        console.print("[yellow]   Set LIFT_SYS_ENABLE_DEMO_USER_HEADER=1[/yellow]")

    return SessionClient(base_url=api_url, headers=headers)
```

**Success Criteria:**
- âœ… CLI warns when demo mode not set
- âœ… Error messages are helpful, not cryptic

### 4.2 Add Connection Testing in SDK

**File:** `lift_sys/client/session_client.py`

```python
def test_connection(self) -> bool:
    """Test API connection and auth.

    Returns:
        bool: True if connection successful

    Raises:
        ConnectionError: If API unreachable
        AuthenticationError: If auth fails
    """
    try:
        response = httpx.get(
            f"{self.base_url}/health",
            headers=self.headers,
            timeout=self.timeout
        )
        if response.status_code == 401:
            raise AuthenticationError(
                "API requires authentication. "
                "Set LIFT_SYS_ENABLE_DEMO_USER_HEADER=1 for demo mode."
            )
        response.raise_for_status()
        return True
    except httpx.ConnectError as e:
        raise ConnectionError(
            f"Cannot connect to API at {self.base_url}. "
            "Is the server running?"
        ) from e
```

**Success Criteria:**
- âœ… SDK provides `test_connection()` method
- âœ… Clear error messages for common failures
- âœ… Users can diagnose issues quickly

---

## Phase 5: Add E2E Tests (v0.3.0) - **Priority: LOW**
**Timeline:** 1 week
**Goal:** Comprehensive workflow testing

### 5.1 Add Playwright E2E Tests

**Dependencies:**
```bash
uv add --dev playwright pytest-playwright
playwright install
```

**Test:** `tests/e2e/test_web_ui.py`

```python
def test_complete_workflow(page):
    """Test complete web UI workflow."""
    page.goto("http://localhost:5173")

    # Navigate to Prompt Workbench
    page.click("text=Prompt Workbench")

    # Create session
    page.fill("textarea[placeholder*='prompt']", "Test function")
    page.click("button:has-text('Create Session')")

    # Wait for ambiguities to appear
    page.wait_for_selector("[data-testid='ambiguity-badge']")

    # Resolve a hole
    page.click("[data-testid='ambiguity-badge']:first")
    page.fill("input[data-testid='resolution-input']", "test_value")
    page.click("button:has-text('Resolve')")

    # Verify success
    assert page.locator("text=Resolved").is_visible()
```

**Success Criteria:**
- âœ… E2E tests cover all workflows
- âœ… Tests run in CI/CD
- âœ… Frontend regressions caught early

### 5.2 Add Textual TUI Tests

**Dependencies:**
```bash
uv add --dev textual[dev]
```

**Test:** `tests/e2e/test_tui.py`

```python
from textual.testing import TUITestCase

class TestTUI(TUITestCase):
    def test_session_creation(self):
        """Test TUI session creation."""
        app = LiftSysApp()

        async with app.run_test() as pilot:
            # Navigate to Prompt Refinement tab
            await pilot.press("tab")

            # Enter prompt
            await pilot.click("#prompt-input")
            await pilot.press("T", "e", "s", "t")
            await pilot.press("ctrl+enter")

            # Verify session appears
            assert pilot.app.state.active_session is not None
```

**Success Criteria:**
- âœ… TUI tests cover key workflows
- âœ… Tests validate UI state
- âœ… Regressions caught before release

---

## Phase 6: Performance & Monitoring (v0.3.0) - **Priority: LOW**
**Timeline:** 1 week
**Goal:** Production-grade observability

### 6.1 Add Performance Benchmarks

**Test:** `tests/performance/test_benchmarks.py`

```python
def test_session_creation_performance(benchmark):
    """Benchmark session creation time."""
    client = SessionClient()

    result = benchmark(
        client.create_session,
        prompt="Performance test function"
    )

    assert result.session_id
    # Should complete in < 500ms
    assert benchmark.stats['mean'] < 0.5
```

**Metrics to Track:**
- Session creation time
- Hole resolution time
- SMT verification time
- Finalization time
- API response times (p50, p95, p99)

**Success Criteria:**
- âœ… Baseline metrics established
- âœ… Performance regressions detected
- âœ… SLA targets defined

### 6.2 Add Structured Logging

**File:** `lift_sys/api/server.py`

```python
import structlog

logger = structlog.get_logger()

@app.post("/spec-sessions")
async def create_session(...):
    logger.info(
        "session.create.started",
        source=request.source,
        prompt_length=len(request.prompt) if request.prompt else 0
    )

    # ... create session ...

    logger.info(
        "session.create.completed",
        session_id=session.session_id,
        ambiguities_count=len(session.ambiguities),
        duration_ms=elapsed_ms
    )
```

**Success Criteria:**
- âœ… All operations logged with context
- âœ… Logs are structured (JSON)
- âœ… Easy to query and analyze

### 6.3 Add Metrics Endpoint

**File:** `lift_sys/api/server.py`

```python
from prometheus_client import Counter, Histogram, generate_latest

session_creates = Counter('session_creates_total', 'Total session creations')
session_duration = Histogram('session_duration_seconds', 'Session creation duration')

@app.get("/metrics")
async def metrics():
    """Prometheus metrics endpoint."""
    return Response(generate_latest(), media_type="text/plain")
```

**Success Criteria:**
- âœ… Key metrics exposed
- âœ… Prometheus-compatible
- âœ… Dashboards can be built

---

## Implementation Priority & Timeline

### Immediate (Next 2-3 days) - v0.2.1 Release

| Task | Priority | Effort | Owner | Status |
|------|----------|--------|-------|--------|
| 1.1 Fix CLI Auth | ðŸ”´ Critical | 2h | - | Pending |
| 1.2 Fix SDK Auth | ðŸ”´ Critical | 1h | - | Pending |
| 1.3 Fix CLI Warning | ðŸŸ¡ High | 1h | - | Pending |
| 1.4 Fix datetime | âš ï¸ Medium | 1h | - | Pending |
| 1.5 Fix FastAPI lifespan | âš ï¸ Medium | 30m | - | Pending |
| 2.1 Fix API test | ðŸŸ¡ High | 1h | - | Pending |
| 2.2 Fix CLI tests | ðŸŸ¡ High | 2h | - | Pending |
| 2.3 Fix TUI tests | âš ï¸ Medium | 3h | - | Pending |

**Total Effort:** ~12 hours (1.5 days)
**Goal:** Make system usable, achieve 100% test pass rate

### Short-Term (Next 1-2 weeks) - v0.2.2 Polish

| Task | Priority | Effort | Owner | Status |
|------|----------|--------|-------|--------|
| 3.1 Update README | ðŸŸ¡ Medium | 30m | - | Pending |
| 3.2 Update Workflows | ðŸŸ¡ Medium | 1h | - | Pending |
| 3.3 Update FAQ | ðŸŸ¡ Medium | 1h | - | Pending |
| 4.1 CLI error messages | âš ï¸ Medium | 2h | - | Pending |
| 4.2 SDK connection testing | âš ï¸ Medium | 2h | - | Pending |

**Total Effort:** ~7 hours (1 day)
**Goal:** Better documentation and UX

### Long-Term (Next 1-2 months) - v0.3.0 Features

| Task | Priority | Effort | Owner | Status |
|------|----------|--------|-------|--------|
| 5.1 Playwright E2E | ðŸŸ¢ Low | 3 days | - | Pending |
| 5.2 Textual TUI tests | ðŸŸ¢ Low | 2 days | - | Pending |
| 6.1 Performance benchmarks | ðŸŸ¢ Low | 2 days | - | Pending |
| 6.2 Structured logging | ðŸŸ¢ Low | 2 days | - | Pending |
| 6.3 Metrics endpoint | ðŸŸ¢ Low | 1 day | - | Pending |

**Total Effort:** ~10 days
**Goal:** Production-grade reliability

---

## Success Metrics

### v0.2.1 Release Criteria

- âœ… **100% unit tests passing** (currently 100%)
- âœ… **100% integration tests passing** (currently 62%)
- âœ… **CLI functional** with demo mode (currently broken)
- âœ… **SDK functional** with demo mode (currently broken)
- âœ… **0 critical bugs** (currently 3)
- âœ… **< 100 deprecation warnings** (currently 876)

### v0.3.0 Release Criteria

- âœ… **E2E tests implemented** (currently 0%)
- âœ… **Performance baselines established**
- âœ… **Structured logging implemented**
- âœ… **Metrics endpoint available**
- âœ… **< 10 deprecation warnings** (currently 876)

---

## Risk Assessment

### High Risk (Blocks Release)

1. **CLI auth fix complexity** - May have unforeseen dependencies
   - **Mitigation:** Thorough testing, rollback plan

2. **Integration test fixes** - May uncover more bugs
   - **Mitigation:** Fix issues as discovered, extend timeline if needed

### Medium Risk (Delays Features)

3. **TUI test refactoring** - May require significant rework
   - **Mitigation:** Consider removing broken tests, add E2E later

4. **Performance issues** - Unknown current baseline
   - **Mitigation:** Establish benchmarks first, optimize later

### Low Risk (Documentation Only)

5. **Documentation updates** - Low impact if delayed
   - **Mitigation:** Can be done incrementally

---

## Conclusion

The system is **close to production-ready** but requires **critical auth fixes** before release. With focused effort over 1-2 days, v0.2.1 can be released with full functionality.

**Recommended Path:**
1. **Immediate:** Fix CLI and SDK auth (Phase 1)
2. **Short-term:** Fix all integration tests (Phase 2)
3. **Medium-term:** Update documentation (Phase 3)
4. **Long-term:** Add E2E tests and monitoring (Phases 5-6)

**Estimated Time to Production:**
- **v0.2.1 (usable):** 2-3 days
- **v0.2.2 (polished):** 1 week
- **v0.3.0 (production-grade):** 1-2 months

---

**Plan Version:** 1.0
**Last Updated:** October 11, 2025
**Next Review:** After v0.2.1 release
