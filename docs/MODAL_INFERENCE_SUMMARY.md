# Modal Inference: Complete Summary
**Date**: October 15, 2025
**Status**: Working Configuration Documented + Upgrade Paths Researched

---

## Current Status: ‚úÖ WORKING

**Production-Ready Stack**:
- vLLM 0.9.2 + XGrammar 0.1.19
- Qwen2.5-Coder-7B-Instruct
- A10G GPU (24GB) @ ~$1.10/hr
- transformers 4.53.0
- CUDA 12.4.1

**Performance**:
- Generation: ~6.4s per request (warm)
- Tokens: ~20-22 tps
- Quality: Valid IR JSON, schema-compliant

**Documentation**: See `docs/MODAL_WORKING_CONFIG.md`

---

## Quick Reference Documents

### 1. MODAL_WORKING_CONFIG.md
**Purpose**: Known working configuration for vLLM + XGrammar
**Use when**: You need the exact versions and setup that works
**Key info**:
- vLLM 0.9.2 API changes (`GuidedDecodingParams` instead of `guided_json`)
- transformers version constraint (4.51.1 - 4.53.x)
- Common errors and solutions

### 2. QWEN3_CODER_GUIDE.md
**Purpose**: How to upgrade to Qwen3-Coder models (better quality)
**Use when**: Current quality isn't sufficient or you want to test newer models
**Key info**:
- Qwen3-30B-A3B-Instruct-FP8 on A100-40GB (~$3/hr)
- vLLM support confirmed (>= 0.8.4)
- Migration path from Qwen2.5-7B

### 3. SGLANG_MODAL_ISSUES.md
**Purpose**: SGLang investigation and troubleshooting
**Use when**: Need 3-10x faster generation or investigating performance optimization
**Key info**:
- Previous sgl_kernel errors on H100
- Modal's recommended SGLang config (CUDA 12.8.0, Python 3.11)
- Investigation tasks if pursuing SGLang path

---

## Upgrade Paths

### Path 1: Add FlashInfer (Easy Win) üéØ RECOMMENDED FIRST
**Benefit**: 10-20% faster sampling
**Cost**: None (same GPU)
**Risk**: Very low
**Effort**: 1 line change

```python
.pip_install(
    "vllm==0.9.2",
    "xgrammar",
    "flashinfer",  # ‚Üê Add this
    "transformers==4.53.0",
    # ...
)
```

**Expected**: 6.4s ‚Üí ~5.5s per request

---

### Path 2: Upgrade to Qwen3-30B-FP8 (Better Quality)
**Benefit**: Significantly better code understanding
**Cost**: ~$3/hr (vs $1.10/hr) - 2.7x increase
**Risk**: Low (same vLLM stack)
**Effort**: Change model name, upgrade GPU

```python
MODEL_NAME = "Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8"
GPU_CONFIG = "A100"  # 40GB variant
```

**When to do**:
- Quality improvement justifies cost
- Need 256K context (vs 32K)
- Production workload is quality-critical

**Documentation**: `docs/QWEN3_CODER_GUIDE.md`

---

### Path 3: Try SGLang (Maximum Performance)
**Benefit**: 3-10x faster constrained generation
**Cost**: Same as Qwen3 (~$3/hr for A100)
**Risk**: Medium (sgl_kernel compatibility unknown)
**Effort**: Significant investigation

**Recommended Approach**:
1. Try Modal's exact configuration first (CUDA 12.8.0, Python 3.11, SGLang 0.4.10.post2)
2. If that fails, try Modal Labs fork
3. If both fail, stick with vLLM

**When to do**:
- Request volume is high
- Latency is critical
- Cost per request matters more than hourly rate

**Documentation**: `docs/SGLANG_MODAL_ISSUES.md`

---

## Recommended Timeline

### ‚úÖ Week 1: Baseline (DONE)
- Qwen2.5-7B on A10G with vLLM 0.9.2
- XGrammar working
- Production-ready endpoint

### üéØ Week 2: Quick Wins
- [x] Add FlashInfer (‚úÖ COMPLETE - 3.49s vs 6-7s baseline)
- [ ] Test with production IR prompts
- [ ] Measure real-world quality metrics

### üìä Week 3-4: Quality Testing (Optional)
- [ ] Test Qwen3-30B-A3B-Instruct-FP8 on A100
- [ ] Side-by-side quality comparison
- [ ] Cost/benefit analysis
- [ ] Decide: Upgrade or stay with Qwen2.5-7B

### üî¨ Week 5+: Performance Optimization (Optional)
- [ ] If needed: Investigate SGLang with Modal's config
- [ ] Measure actual performance gains
- [ ] Decide: Worth the complexity?

---

## Decision Tree

```
Current: Qwen2.5-7B on A10G (working)
    ‚îÇ
    ‚îú‚îÄ‚Üí Quality good enough?
    ‚îÇ       YES ‚Üí Add FlashInfer, ship to production ‚úÖ
    ‚îÇ       NO ‚Üí Continue below
    ‚îÇ
    ‚îú‚îÄ‚Üí Need better quality?
    ‚îÇ       YES ‚Üí Test Qwen3-30B-FP8 on A100 üìä
    ‚îÇ       NO ‚Üí Continue below
    ‚îÇ
    ‚îî‚îÄ‚Üí Need faster generation?
            YES ‚Üí Investigate SGLang üî¨
            NO ‚Üí Ship current setup ‚úÖ
```

---

## Cost Analysis

| Configuration | GPU | Cost/hr | Est. Cost/1K Requests | Use Case |
|---------------|-----|---------|----------------------|----------|
| **Qwen2.5-7B** | A10G | $1.10 | ~$2-3 | Development, MVP |
| **Qwen2.5-7B + FlashInfer** | A10G | $1.10 | ~$1.70-2.50 | Production (best cost) |
| **Qwen3-30B-FP8** | A100-40GB | $3.00 | ~$5-8 | Quality-critical |
| **Qwen3-30B + SGLang** | A100-40GB | $3.00 | ~$0.50-2 | High volume |

*Estimates based on 6s/request (current) with expected improvements*

---

## Key Learnings

### vLLM 0.9.2 API Changes
‚ùå **Old** (0.6.x):
```python
sampling_params = SamplingParams(guided_json=schema)
```

‚úÖ **New** (0.9.2+):
```python
from vllm.sampling_params import GuidedDecodingParams
guided_decoding = GuidedDecodingParams(json=schema)
sampling_params = SamplingParams(guided_decoding=guided_decoding)
```

### transformers Version Window
- **Minimum**: 4.51.1 (vLLM 0.9.2 requirement)
- **Maximum**: 4.53.x (aimv2 conflict at 4.54.0+)
- **Sweet spot**: 4.53.0 ‚úÖ

### Modal FastAPI Endpoints
Can't call `@modal.method()` from `@modal.fastapi_endpoint()` directly:

```python
# Solution: Separate implementation
def _generate_impl(self, ...):  # Private method
    # Implementation

@modal.method()
def generate(self, ...):
    return self._generate_impl(...)  # For .remote() calls

@modal.fastapi_endpoint()
async def web_generate(self, item):
    return self._generate_impl(...)  # Direct call
```

---

## Testing Checklist

### Before Deploying Changes
- [ ] Test with `modal serve` (hot-reload)
- [ ] Verify health endpoint responds
- [ ] Test generate endpoint with sample IR
- [ ] Check logs for warnings/errors
- [ ] Measure actual latency
- [ ] Validate JSON schema compliance

### Before Upgrading GPU
- [ ] Confirm model size fits in VRAM
- [ ] Check Modal GPU availability
- [ ] Calculate cost impact
- [ ] Have rollback plan

### Before Changing Models
- [ ] Test with representative prompts
- [ ] Compare quality metrics
- [ ] Measure latency differences
- [ ] Validate schema compliance still works

---

## Troubleshooting Quick Reference

### Issue: "Unexpected keyword argument 'guided_json'"
**Fix**: Update to GuidedDecodingParams API (vLLM 0.9.2+)
**Doc**: MODAL_WORKING_CONFIG.md

### Issue: "aimv2 already used by Transformers config"
**Fix**: Downgrade transformers to 4.53.0
**Doc**: MODAL_WORKING_CONFIG.md

### Issue: "'Function' object is not callable"
**Fix**: Use `_generate_impl()` pattern for shared code
**Doc**: MODAL_WORKING_CONFIG.md

### Issue: "FlashInfer is not available"
**Fix**: Add `flashinfer` to pip_install
**Doc**: MODAL_WORKING_CONFIG.md

### Issue: "sgl_kernel ImportError"
**Fix**: Try Modal's exact SGLang config or use vLLM
**Doc**: SGLANG_MODAL_ISSUES.md

---

## Resources

### Documentation Created
- ‚úÖ MODAL_WORKING_CONFIG.md - Known working setup
- ‚úÖ QWEN3_CODER_GUIDE.md - Upgrade paths
- ‚úÖ SGLANG_MODAL_ISSUES.md - SGLang investigation
- ‚úÖ MODAL_INFERENCE_SUMMARY.md - This file

### External Resources
- vLLM Docs: https://docs.vllm.ai/en/v0.9.2/
- XGrammar: https://xgrammar.mlc.ai
- Modal LLM Guide: https://modal.com/docs/guide/developing-with-llms
- FlashInfer: https://github.com/flashinfer-ai/flashinfer
- Qwen3-Coder: https://github.com/QwenLM/Qwen3-Coder

---

## Next Actions Priority

1. **‚úÖ DONE: Add FlashInfer** (Achieved 3.49s vs 6-7s baseline)
2. **üìä MEDIUM: Test Qwen3-30B** (if quality isn't sufficient)
3. **üî¨ LOW: Investigate SGLang** (only if performance is critical)

**Recommendation**: Current setup with FlashInfer is production-ready. Test with real IR prompts, then ship to production.

---

**Last Updated**: October 15, 2025
**Status**: Documentation complete, ready for next phase
**Owner**: Claude Code
