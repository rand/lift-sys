# ACE Research Analysis: Agentic Context Engineering

**Research Date**: 2025-10-15
**Researcher**: Claude (Sonnet 4.5)
**Project**: lift-sys Enhancement Analysis
**Status**: Research Only - No Code Changes

---

## Executive Summary

**ACE** (Agentic Context Engineering) is a framework for adaptive context management in LLMs that treats contexts as "evolving playbooks" through a three-role architecture (Generator/Reflector/Curator). Published at arXiv:2510.04618 (October 2025) by Stanford/SambaNova/UC Berkeley researchers.

**Key Finding**: ACE is **HIGHLY RELEVANT** to lift-sys. Its approach to iterative context refinement, structured knowledge accumulation, and prevention of information degradation directly addresses challenges we face in iterative IR refinement.

**Recommendation**: **Adopt core ACE concepts** in our Semantic IR implementation, particularly for:
- Iterative prompt ‚Üí IR refinement (Phase 3)
- Knowledge accumulation across sessions
- Preventing "spec collapse" during multiple refinement iterations
- Tracking inference quality with helpful/harmful counters

**Impact**: Medium-High implementation effort (2-3 weeks), but could significantly improve Phase 3 (Interactive Refinement) user experience and system quality.

---

## 1. ACE Overview

### 1.1 What is ACE?

ACE addresses two critical problems in LLM context adaptation:

1. **Brevity Bias**: Systems that optimize for concise prompts lose domain-specific insights, tool-use guidelines, and failure mode knowledge
2. **Context Collapse**: Iterative rewriting degrades detail over time, causing performance degradation

**Solution**: Treat contexts as "evolving playbooks" with structured, incremental updates.

### 1.2 Core Architecture: Three Agentic Roles

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Generator  ‚îÇ ‚îÄ‚îÄ‚îÄ> ‚îÇ  Reflector  ‚îÇ ‚îÄ‚îÄ‚îÄ> ‚îÇ   Curator   ‚îÇ
‚îÇ             ‚îÇ      ‚îÇ             ‚îÇ      ‚îÇ             ‚îÇ
‚îÇ Produces    ‚îÇ      ‚îÇ Critiques   ‚îÇ      ‚îÇ Synthesizes ‚îÇ
‚îÇ reasoning   ‚îÇ      ‚îÇ and extracts‚îÇ      ‚îÇ into delta  ‚îÇ
‚îÇ trajectories‚îÇ      ‚îÇ lessons     ‚îÇ      ‚îÇ updates     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                    ‚îÇ                     ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                     ‚îÇ  Playbook   ‚îÇ
                     ‚îÇ  (Context)  ‚îÇ
                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Generator**: Produces reasoning trajectories, surfaces strategies and pitfalls
**Reflector**: Extracts lessons from successes/failures, refines across iterations
**Curator**: Synthesizes lessons into compact "delta entries", merges deterministically

### 1.3 Key Innovation: Delta Updates

Instead of rewriting entire contexts (which causes collapse), ACE uses **incremental delta updates**:

```python
# Conceptual structure
Playbook = {
  "bullets": [
    {
      "id": "bullet_42",
      "content": "When handling lists, always validate non-empty",
      "helpful_count": 15,
      "harmful_count": 2,
      "confidence": 0.88,
      "created": "2025-10-01",
      "last_used": "2025-10-15"
    },
    ...
  ]
}

# Delta update (not full rewrite)
delta = {
  "add": [new_bullet_1, new_bullet_2],
  "update": {
    "bullet_42": {"helpful_count": 16}
  },
  "remove": []
}
```

### 1.4 Performance Results

**AppWorld Agent Tasks**: +10.6% improvement
**Finance Reasoning**: +8.6% improvement
**Adaptation Latency**: -86.9% reduction
**Leaderboard**: Matched top-ranked production agent (smaller model)

### 1.5 Related Work: Dynamic Cheatsheet

ACE builds on **Dynamic Cheatsheet** (arXiv:2504.07952):
- Persistent, evolving memory at inference time
- Compact library of reusable strategies, solution sketches, code snippets
- Self-curated storage focused on transferable knowledge
- **Results**: Claude 3.5 Sonnet's accuracy doubled on AIME math; GPT-4o improved from 10% ‚Üí 99% on Game of 24

**GitHub**: https://github.com/suzgunmirac/dynamic-cheatsheet
**ACE Implementation**: https://github.com/sci-m-wang/ACE-open

---

## 2. Technical Deep Dive

### 2.1 Playbook Structure

**Core Data Model**:
```python
class Playbook:
    """Structured context storage with metadata tracking"""

    bullets: List[Bullet]  # Individual knowledge items
    metadata: Dict         # Global stats

    def add_delta(self, delta: Delta) -> None:
        """Incrementally update without full rewrite"""

    def retrieve(self, query: str, k: int = 5) -> List[Bullet]:
        """Similarity-based retrieval for relevant context"""

    def prune(self, threshold: float = 0.3) -> None:
        """Remove low-quality bullets (harmful > helpful)"""

class Bullet:
    """Atomic knowledge unit"""
    id: str
    content: str                 # Strategy, insight, or pattern
    helpful_count: int           # Successes when used
    harmful_count: int           # Failures when used
    embedding: Optional[Vector]  # For semantic retrieval
    metadata: Dict               # Created, last_used, domain, etc.
```

### 2.2 Three-Role Workflow

#### 2.2.1 Generator Role

**Input**: Task query + Current playbook
**Output**: Reasoning trajectory (chain of thought)

```python
class Generator:
    def generate(self, query: str, playbook: Playbook) -> Trajectory:
        # Retrieve relevant context
        relevant = playbook.retrieve(query, k=5)

        # Construct prompt with context
        prompt = self._build_prompt(query, relevant)

        # Generate solution
        trajectory = self.llm.generate(prompt)

        return trajectory
```

**Prompt Template** (simplified):
```
Relevant context from playbook:
{relevant_bullets}

Task: {query}

Reasoning:
```

#### 2.2.2 Reflector Role

**Input**: Query + Trajectory + Execution feedback (success/failure)
**Output**: Extracted lessons

```python
class Reflector:
    def reflect(self, query: str, trajectory: Trajectory,
                feedback: ExecutionFeedback) -> List[Lesson]:
        # Analyze what worked / didn't work
        prompt = f"""
        Query: {query}
        Reasoning: {trajectory}
        Result: {feedback.status} - {feedback.message}

        Extract 2-3 specific, transferable lessons.
        Focus on concrete strategies, not vague advice.
        """

        lessons = self.llm.generate(prompt)
        return self._parse_lessons(lessons)
```

**Multi-iteration refinement**: Can iterate to improve lesson quality

#### 2.2.3 Curator Role

**Input**: Lessons + Current playbook
**Output**: Delta update

```python
class Curator:
    def curate(self, lessons: List[Lesson],
               playbook: Playbook) -> Delta:
        # Check for duplicates via semantic similarity
        new_bullets = []
        updates = {}

        for lesson in lessons:
            # Find similar existing bullets
            similar = playbook.find_similar(lesson.content, threshold=0.85)

            if similar:
                # Update existing bullet
                updates[similar.id] = {
                    "helpful_count": similar.helpful_count + 1
                }
            else:
                # Add new bullet
                new_bullets.append(Bullet(
                    id=generate_id(),
                    content=lesson.content,
                    helpful_count=1,
                    harmful_count=0,
                    embedding=self.embed(lesson.content)
                ))

        return Delta(add=new_bullets, update=updates, remove=[])
```

### 2.3 Preventing Context Collapse

**Problem**: Iterative rewriting ‚Üí shorter, less informative summaries

**ACE Solutions**:

1. **Incremental Updates**: Never rewrite entire playbook
2. **Bullet-Level Tracking**: Each item has usage counters
3. **Deterministic Merging**: Use embedding similarity, not LLM judgment
4. **Periodic Pruning**: Remove low-quality bullets based on metrics
5. **Structured Storage**: Enforce schema, prevent free-form degradation

**Example**:
```python
# BAD (causes collapse):
new_context = llm.summarize(old_context + new_info)  # Loses detail

# GOOD (ACE approach):
delta = curator.curate(new_lessons, playbook)
playbook.add_delta(delta)  # Preserves all existing bullets
```

### 2.4 Preventing Brevity Bias

**Problem**: Concise summaries drop domain-specific heuristics

**ACE Solutions**:

1. **Accumulation over Compression**: Grow playbook, don't shrink
2. **Domain-Specific Bullets**: Store concrete patterns, not abstractions
3. **Retrieval-Based Context**: Use only relevant bullets per query
4. **Quality Filtering**: Remove harmful patterns, keep helpful details

**Example**:
```python
# BAD (brevity bias):
"Handle edge cases carefully"  # Vague, loses specifics

# GOOD (ACE style):
"When processing user input, check: (1) empty string, (2) null,
 (3) whitespace-only. Return early with clear error messages."
```

---

## 3. Relevance Assessment for lift-sys

### 3.1 Direct Applicability: **HIGH** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

| Aspect | ACE | lift-sys Semantic IR | Match? |
|--------|-----|----------------------|--------|
| **Domain** | Iterative context refinement | Iterative IR refinement | ‚úÖ Strong |
| **Problem** | Prevent context collapse | Prevent spec degradation | ‚úÖ Exact |
| **Approach** | Structured incremental updates | Typed holes + refinement | ‚úÖ Compatible |
| **Use Case** | Accumulate domain knowledge | Accumulate constraints | ‚úÖ Aligned |
| **Architecture** | 3 agentic roles | Multi-stage pipeline | üü° Adaptable |

**Conclusion**: ACE directly addresses problems we'll face in Phase 3 (Interactive Refinement).

### 3.2 Problem-Solution Mapping

#### Problem 1: IR Degradation During Refinement

**lift-sys Challenge**: As users resolve holes, IR could become inconsistent or lose detail

**ACE Solution**: Delta updates preserve existing structure, only modify what changed

**Applicability**: ‚úÖ **HIGH** - Directly solves our problem

#### Problem 2: Accumulating Inference Knowledge

**lift-sys Challenge**: Learning which inference rules work for which contexts

**ACE Solution**: Helpful/harmful counters track rule effectiveness

**Applicability**: ‚úÖ **HIGH** - Complements Phase 2.3 (Inference Rules)

#### Problem 3: Ambiguity Resolution Quality

**lift-sys Challenge**: Ensuring refinement suggestions improve over time

**ACE Solution**: Reflector role learns from user feedback (accepted/rejected)

**Applicability**: ‚úÖ **MEDIUM-HIGH** - Enhances Phase 3.2 (LLM Suggestions)

#### Problem 4: Cross-Session Learning

**lift-sys Challenge**: Each session starts from scratch

**ACE Solution**: Persistent playbook accumulates patterns across sessions

**Applicability**: ‚úÖ **MEDIUM** - Valuable for power users, optional for MVP

---

## 4. Concepts Worth Adopting

### 4.1 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Delta-Based IR Updates (CRITICAL)

**ACE Approach**: Incremental updates, never full rewrites

**Potential for lift-sys**:
```python
# Phase 3.3.1: IR Update Propagation
class IRDeltaUpdate:
    """ACE-inspired delta updates for IR refinement"""

    add: List[IRElement]       # New entities, types, constraints
    update: Dict[str, Any]     # Modified elements (by ID)
    remove: List[str]          # Removed element IDs
    metadata: UpdateMetadata   # Timestamp, user, confidence

    def apply(self, ir: EnhancedIR) -> EnhancedIR:
        """Apply delta to existing IR without full rebuild"""
        # Preserve all unchanged elements
        # Only modify specified deltas
        # Track provenance of changes
```

**Benefits**:
- Prevents "spec collapse" during multi-turn refinement
- Clear audit trail of what changed
- Easy undo/redo (reverse delta)
- Faster updates (no full IR regeneration)

**Integration Point**: Phase 3.3.1 (IR Update Propagation Engine)

**Effort**: 3-4 days (data model + application logic)

**Priority**: üü¢ **P0 - Strongly recommend**

---

### 4.2 ‚≠ê‚≠ê‚≠ê‚≠ê Inference Rule Quality Tracking (HIGH VALUE)

**ACE Approach**: Helpful/harmful counters per bullet

**Potential for lift-sys**:
```python
# Phase 2.3.1: Rule Library Enhancement
@dataclass
class InferenceRule:
    id: str
    pattern: str                # "delete X" ‚Üí "X must exist"
    reasoning_type: ReasoningType
    helpful_count: int = 0      # NEW: Successful applications
    harmful_count: int = 0      # NEW: User rejected/corrected
    confidence: float           # Computed from counters
    domains: List[str]          # Where this rule applies

    def update_feedback(self, accepted: bool):
        """Track user acceptance/rejection"""
        if accepted:
            self.helpful_count += 1
        else:
            self.harmful_count += 1
        self.confidence = self.helpful_count / (self.helpful_count + self.harmful_count + 1)
```

**Benefits**:
- Rules improve with usage
- Low-quality rules naturally pruned
- Domain-specific rule prioritization
- Better suggestion ranking

**Integration Points**:
- Phase 2.3.1 (Rule Library)
- Phase 3.1.4 (Refinement State Management)
- Phase 3.2.3 (Suggestion Ranking)

**Effort**: 2-3 days (add tracking + UI feedback)

**Priority**: üü¢ **P0 - High value**

---

### 4.3 ‚≠ê‚≠ê‚≠ê‚≠ê Three-Role Refinement Architecture (VALUABLE)

**ACE Approach**: Generator/Reflector/Curator separation

**Potential for lift-sys** (adapted):

```python
# Map to our pipeline
class IRGenerator:
    """Generates initial IR from prompt (like ACE Generator)"""
    # Already exists: Phase 1 prompt ‚Üí IR

class IRReflector:
    """Analyzes IR for quality, detects ambiguities (like ACE Reflector)"""
    # Maps to: Phase 2.2 (Ambiguity Detection)

    def reflect(self, ir: EnhancedIR, user_feedback: Optional[Feedback] = None) -> List[Issue]:
        """
        Find issues in current IR:
        - Ambiguities
        - Missing constraints
        - Contradictions
        - Typed holes
        """
        return self.analyze(ir, user_feedback)

class IRCurator:
    """Curates refinement suggestions and applies deltas (like ACE Curator)"""
    # Maps to: Phase 3.2 (LLM Suggestions) + Phase 3.3 (IR Updates)

    def curate_suggestions(self, issues: List[Issue], ir: EnhancedIR) -> List[Suggestion]:
        """Generate ranked suggestions for each issue"""

    def apply_refinement(self, suggestion: Suggestion, ir: EnhancedIR) -> IRDelta:
        """Create delta update from accepted suggestion"""
```

**Benefits**:
- Clear separation of concerns
- Easier testing (mock each role)
- Reusable components
- Aligns with ACE's proven architecture

**Integration Points**:
- Phase 1 (Generator already exists)
- Phase 2.2 (Add Reflector)
- Phase 3.2-3.3 (Add Curator)

**Effort**: 1-2 days (refactor existing code into role pattern)

**Priority**: üü° **P1 - Nice architectural improvement**

---

### 4.4 ‚≠ê‚≠ê‚≠ê Cross-Session Knowledge Persistence (MEDIUM-HIGH VALUE)

**ACE Approach**: Persistent playbook across queries

**Potential for lift-sys**:
```python
# New: User-specific or project-specific knowledge base
class UserPlaybook:
    """Persistent knowledge accumulated across sessions"""

    user_id: str
    inference_patterns: List[InferenceRule]  # Learned patterns
    domain_vocabulary: Dict[str, Type]       # Domain-specific types
    common_constraints: List[Constraint]     # Reusable constraints

    def update_from_session(self, session: RefinementSession):
        """Extract reusable knowledge from completed session"""
        # Successful inferences ‚Üí add to patterns
        # User-defined types ‚Üí add to vocabulary
        # Frequently used constraints ‚Üí add to library
```

**Example**:
```
Session 1: User defines "EmailAddress" type with validation
   ‚Üí Playbook learns: EmailAddress = str with regex pattern

Session 2: User mentions "email" in new prompt
   ‚Üí System suggests: "Did you mean EmailAddress type from previous session?"
```

**Benefits**:
- Faster refinement (reuse past knowledge)
- Consistency across sessions
- Reduced repetitive explanations
- Power user productivity boost

**Integration Points**:
- Phase 1.1.2 (Database Schema) - store playbooks
- Phase 3.2.1 (LLM Suggestions) - retrieve from playbook
- Phase 3.1.4 (State Management) - link sessions to playbooks

**Effort**: 4-5 days (storage + retrieval + UI)

**Priority**: üü° **P1-P2 - Post-MVP feature**

---

### 4.5 ‚≠ê‚≠ê‚≠ê Semantic Deduplication (MEDIUM VALUE)

**ACE Approach**: Use embeddings to detect duplicate bullets

**Potential for lift-sys**:
```python
# Phase 3.2.3: Suggestion Ranking Enhancement
class SuggestionDeduplicator:
    """Prevent redundant suggestions"""

    def __init__(self, embedding_model: str = "text-embedding-3-small"):
        self.embed = load_embedding_model(embedding_model)

    def deduplicate(self, suggestions: List[Suggestion],
                   threshold: float = 0.90) -> List[Suggestion]:
        """Remove semantically duplicate suggestions"""

        unique = []
        for sugg in suggestions:
            emb = self.embed(sugg.text)

            # Check similarity to existing
            is_duplicate = any(
                cosine_similarity(emb, self.embed(u.text)) > threshold
                for u in unique
            )

            if not is_duplicate:
                unique.append(sugg)

        return unique
```

**Benefits**:
- Cleaner suggestion lists
- Better UX (no redundant options)
- Faster decision-making

**Integration Point**: Phase 3.2.3 (Suggestion Ranking)

**Effort**: 1-2 days (embedding + dedup logic)

**Priority**: üü° **P1 - Nice-to-have**

---

### 4.6 ‚≠ê‚≠ê Reflection-Based Quality Improvement (MEDIUM VALUE)

**ACE Approach**: Reflector iterates to improve lesson quality

**Potential for lift-sys**:
```python
# Phase 3.2.2: LLM Suggestion Enhancement
class SuggestionRefiner:
    """Multi-pass refinement of suggestions"""

    async def refine(self, initial_suggestion: str,
                    context: IRContext,
                    max_iterations: int = 2) -> Suggestion:
        """
        Iteration 1: Generate suggestion
        Iteration 2: Critique and improve
        """
        suggestion = initial_suggestion

        for i in range(max_iterations - 1):
            critique = await self._critique(suggestion, context)
            suggestion = await self._improve(suggestion, critique)

        return suggestion
```

**Benefits**:
- Higher quality suggestions
- Fewer user corrections
- Better explanations

**Trade-offs**:
- 2x latency (2 LLM calls)
- 2x cost

**Integration Point**: Phase 3.2.2 (LLM Integration Layer)

**Effort**: 2 days

**Priority**: üü¢ **P2 - Optional quality improvement**

---

## 5. Concepts NOT Worth Adopting

### 5.1 Full Multi-Epoch Training Loop

**Why Not**: ACE's OfflineAdapter runs multiple epochs for training. lift-sys is interactive, not batch training.

**Alternative**: Use online adaptation (accept/reject feedback) instead.

### 5.2 Task-Specific Benchmarks

**Why Not**: ACE evaluates on AppWorld and finance datasets. Our evaluation is user-driven (usability, correctness).

**Alternative**: User acceptance testing, A/B testing in Phase 6.

### 5.3 Heavyweight Embedding Infrastructure

**Why Not**: ACE uses dense retrieval with embeddings. Adds complexity.

**When to Adopt**: Only if cross-session persistence (4.4) is implemented and user base grows.

---

## 6. Proposed Implementation Plan

### 6.1 High-Priority Enhancements (Phase 3 Focus)

#### Enhancement A: Delta-Based IR Updates ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**What**: Replace full IR rewrites with incremental delta updates

**Why**: Prevents spec degradation, clearer audit trail, enables undo/redo

**Where**: Phase 3.3.1 (IR Update Propagation Engine)

**How**:
1. Define `IRDelta` data model (add/update/remove)
2. Implement `apply_delta(ir: EnhancedIR, delta: IRDelta) -> EnhancedIR`
3. Update all refinement operations to produce deltas
4. Track delta history for provenance

**Effort**: 3-4 days

**Priority**: P0 (Critical for Phase 3 quality)

**New Bead**: lift-sys-167

---

#### Enhancement B: Inference Rule Quality Tracking ‚≠ê‚≠ê‚≠ê‚≠ê

**What**: Add helpful/harmful counters to inference rules

**Why**: Rules improve with usage, better suggestion ranking

**Where**:
- Phase 2.3.1 (Rule Library)
- Phase 3.2.3 (Suggestion Ranking)

**How**:
1. Add `helpful_count`, `harmful_count`, `confidence` to `InferenceRule`
2. Implement feedback collection (user accepts/rejects suggestions)
3. Update suggestion ranking to use confidence scores
4. Add rule pruning logic (remove low-confidence rules)

**Effort**: 2-3 days

**Priority**: P0 (High impact on suggestion quality)

**New Bead**: lift-sys-168

---

#### Enhancement C: Three-Role Architecture Refactor ‚≠ê‚≠ê‚≠ê

**What**: Refactor existing pipeline into Generator/Reflector/Curator roles

**Why**: Clearer architecture, easier testing, aligns with proven pattern

**Where**:
- Phase 1 (Generator)
- Phase 2.2 (Reflector)
- Phase 3.2-3.3 (Curator)

**How**:
1. Create `IRGenerator` class (wrap existing prompt‚ÜíIR)
2. Create `IRReflector` class (wrap ambiguity detection)
3. Create `IRCurator` class (wrap suggestion generation + delta application)
4. Update orchestration to use three roles

**Effort**: 1-2 days (mostly refactoring)

**Priority**: P1 (Architectural improvement)

**New Bead**: lift-sys-169

---

### 6.2 Medium-Priority Enhancements (Post-Phase 3)

#### Enhancement D: Semantic Suggestion Deduplication ‚≠ê‚≠ê‚≠ê

**What**: Use embeddings to remove duplicate suggestions

**Effort**: 1-2 days

**Priority**: P1

**New Bead**: lift-sys-170

---

#### Enhancement E: Cross-Session Knowledge Persistence ‚≠ê‚≠ê‚≠ê

**What**: Persistent user/project playbook

**Effort**: 4-5 days

**Priority**: P2 (Post-MVP)

**New Bead**: lift-sys-171

---

#### Enhancement F: Multi-Pass Suggestion Refinement ‚≠ê‚≠ê

**What**: Reflector-style iteration on suggestions

**Effort**: 2 days

**Priority**: P2 (Optional quality improvement)

**New Bead**: lift-sys-172

---

### 6.3 Updated Task Summary

**Total New Beads**: 6 (lift-sys-167 to lift-sys-172)

**Phase 3 Impact** (High-Priority):
- lift-sys-167: Delta-Based IR Updates (3-4 days, P0)
- lift-sys-168: Inference Rule Quality Tracking (2-3 days, P0)
- lift-sys-169: Three-Role Architecture (1-2 days, P1)

**Subtotal**: 6-9 days (1.2-1.8 weeks)

**Post-Phase 3** (Medium-Priority):
- lift-sys-170: Semantic Deduplication (1-2 days, P1)
- lift-sys-171: Cross-Session Persistence (4-5 days, P2)
- lift-sys-172: Multi-Pass Refinement (2 days, P2)

**Subtotal**: 7-9 days (1.4-1.8 weeks)

**Total Additional Effort**: 13-18 days (2.6-3.6 weeks)

**Impact on 52-week plan**: ~5-7% increase

**Recommendation**: Adopt A, B, C (high-priority) in Phase 3. Defer D, E, F to Phase 6 or post-MVP based on user feedback.

---

## 7. Integration with Existing Plan

### 7.1 Phase 3 (Interactive Refinement) - MAJOR ENHANCEMENT

**Existing Tasks**:
- lift-sys-104 to lift-sys-118 (15 tasks)

**New ACE-Enhanced Tasks**:
- **lift-sys-167**: Insert before lift-sys-112 (IR Update Propagation)
  - Implements delta-based updates
  - Makes lift-sys-112 use deltas instead of full rewrites

- **lift-sys-168**: Insert after lift-sys-96 (Rule Library)
  - Adds quality tracking to rules
  - Feeds into lift-sys-110 (Suggestion Ranking)

- **lift-sys-169**: Insert after lift-sys-107 (State Management)
  - Refactors into three-role architecture
  - Makes lift-sys-108-111 use role pattern

**Dependencies**:
```
lift-sys-96 (Rule Library)
  ‚Üì
lift-sys-168 (Rule Quality Tracking) ‚Üê NEW
  ‚Üì
lift-sys-110 (Suggestion Ranking)

lift-sys-107 (State Management)
  ‚Üì
lift-sys-169 (Three-Role Architecture) ‚Üê NEW
  ‚Üì
lift-sys-108-111 (Suggestion System)

lift-sys-112 (IR Update Propagation)
  ‚Üì
lift-sys-167 (Delta-Based Updates) ‚Üê NEW (replaces full rewrites)
  ‚Üì
lift-sys-113 (Consistency Checker)
```

### 7.2 Phase 6 (Polish & Deployment) - OPTIONAL ENHANCEMENTS

**Existing Tasks**:
- lift-sys-146 to lift-sys-162 (17 tasks)

**Optional ACE Enhancements** (defer to post-MVP):
- **lift-sys-170**: Semantic Deduplication (after lift-sys-147 - Frontend Optimization)
- **lift-sys-171**: Cross-Session Persistence (after lift-sys-149 - Caching Strategy)
- **lift-sys-172**: Multi-Pass Refinement (after lift-sys-109 - LLM Integration)

---

## 8. Key Takeaways

### ‚úÖ Strongly Adopt (P0):
1. **Delta-Based IR Updates** - Prevents spec collapse, critical for quality
2. **Inference Rule Quality Tracking** - Improves suggestions over time
3. **Three-Role Architecture** - Proven pattern, clearer design

### üü° Consider (P1-P2):
4. **Semantic Deduplication** - Better UX, low effort
5. **Cross-Session Persistence** - Power user feature, defer to post-MVP
6. **Multi-Pass Refinement** - Quality improvement, but 2x cost

### ‚ùå Skip:
- Multi-epoch training loops (not interactive)
- Heavyweight benchmark infrastructure (different evaluation model)

---

## 9. Comparison: ACE vs. MuSLR

| Aspect | ACE | MuSLR | Winner for lift-sys |
|--------|-----|-------|---------------------|
| **Relevance** | Iterative refinement | Multimodal reasoning | ‚úÖ ACE |
| **Applicability** | Direct (context = IR) | Indirect (logic ‚â† code) | ‚úÖ ACE |
| **Implementation** | Open-source Python | Benchmark dataset | ‚úÖ ACE |
| **Impact** | High (core problem) | Low (orthogonal) | ‚úÖ ACE |
| **Effort** | Medium (2-3 weeks) | Low (6 days) | üü° Tie |

**Conclusion**: ACE is significantly more valuable than MuSLR for lift-sys.

---

## 10. Conclusion

**ACE provides a battle-tested framework for the exact problem we face in Phase 3: iterative refinement without quality degradation.**

The core concepts (delta updates, quality tracking, role-based architecture) are:
- ‚úÖ **Proven**: +10.6% on real benchmarks
- ‚úÖ **Implementable**: Open-source reference
- ‚úÖ **Aligned**: Directly addresses our challenges
- ‚úÖ **Scalable**: Works with long-context models

**Recommendation**:
- **Adopt** high-priority enhancements (A, B, C) in Phase 3
- **Defer** medium-priority enhancements (D, E, F) to Phase 6 or post-MVP
- **Track** as 6 new Beads items (lift-sys-167 to lift-sys-172)

**Net Impact**: ACE concepts will significantly improve Phase 3 quality and user experience, with manageable implementation effort (~2-3 weeks for high-priority items).

---

## 11. References

- **ACE Paper**: https://arxiv.org/abs/2510.04618
- **ACE GitHub**: https://github.com/sci-m-wang/ACE-open
- **Dynamic Cheatsheet Paper**: https://arxiv.org/abs/2504.07952
- **Dynamic Cheatsheet GitHub**: https://github.com/suzgunmirac/dynamic-cheatsheet
- **HuggingFace**: https://huggingface.co/papers/2510.04618

---

**Next Steps**:
1. ‚úÖ Research documented
2. ‚è≠Ô∏è User decision: Adopt high-priority enhancements?
3. ‚è≠Ô∏è If yes: Create Beads items lift-sys-167, 168, 169 (Phase 3)
4. ‚è≠Ô∏è Optional: Create Beads items lift-sys-170, 171, 172 (Phase 6/post-MVP)

**Status**: Research complete, awaiting user feedback.
