"""Generate a comprehensive markdown comparison report from benchmark results.

This script reads benchmark JSON output and generates a detailed markdown report
comparing generated TypeScript code against human baselines, including:

- Executive summary with key metrics
- Quality score breakdown by category
- Detailed function-by-function comparisons
- Side-by-side code examples
- Recommendations for improvement
"""

from __future__ import annotations

import argparse
import json
from datetime import datetime
from pathlib import Path
from typing import Any


def generate_markdown_report(results_file: Path, output_file: Path) -> None:
    """
    Generate markdown report from benchmark JSON results.

    Args:
        results_file: Path to benchmark JSON results
        output_file: Path to output markdown file
    """
    # Load results
    with open(results_file) as f:
        data = json.load(f)

    summary = data["summary"]
    results = data["results"]

    # Build markdown report
    md = []

    # Header
    md.append("# TypeScript Generator Quality Comparison Report")
    md.append("")
    md.append(f"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    md.append(f"**Source Data**: `{results_file.name}`")
    md.append("")

    # Executive Summary
    md.append("## Executive Summary")
    md.append("")
    md.append(
        f"This report compares TypeScript code generated by the lift-sys generator "
        f"against {summary['total_functions']} human-written baseline functions."
    )
    md.append("")
    md.append("### Key Metrics")
    md.append("")
    md.append("| Metric | Value | Target | Status |")
    md.append("|--------|-------|--------|--------|")

    avg_quality = summary["avg_quality_score"]
    quality_status = "✅" if avg_quality >= 70 else "⚠️" if avg_quality >= 50 else "❌"
    md.append(f"| **Overall Quality Score** | {avg_quality:.1f}/100 | 70+ | {quality_status} |")

    avg_similarity = summary["avg_structure_similarity"]
    sim_status = "✅" if avg_similarity >= 60 else "⚠️" if avg_similarity >= 40 else "❌"
    md.append(f"| **Structure Similarity** | {avg_similarity:.1f}% | 60%+ | {sim_status} |")

    avg_complexity = summary["avg_complexity_ratio"]
    complex_status = "✅" if avg_complexity <= 1.5 else "⚠️" if avg_complexity <= 2.0 else "❌"
    md.append(f"| **Complexity Ratio** | {avg_complexity:.2f}x | ≤1.5x | {complex_status} |")

    avg_concise = summary["avg_conciseness_ratio"]
    concise_status = "✅" if avg_concise <= 1.5 else "⚠️" if avg_concise <= 2.0 else "❌"
    md.append(f"| **Conciseness Ratio** | {avg_concise:.2f}x | ≤1.5x | {concise_status} |")

    success_rate = (summary["successful"] / summary["total_functions"]) * 100
    success_status = "✅" if success_rate >= 90 else "⚠️" if success_rate >= 70 else "❌"
    md.append(f"| **Success Rate** | {success_rate:.1f}% | 90%+ | {success_status} |")

    md.append("")

    # Quality Distribution
    md.append("### Quality Distribution")
    md.append("")
    dist = summary["quality_distribution"]
    total_successful = summary["successful"]

    for grade in ["excellent", "good", "fair", "poor"]:
        count = dist[grade]
        pct = (count / total_successful * 100) if total_successful > 0 else 0
        bar = "█" * int(pct / 5)  # One block per 5%
        md.append(f"- **{grade.capitalize()}** (80-100): {count} ({pct:.1f}%) {bar}")

    md.append("")

    # Category Performance
    md.append("## Performance by Category")
    md.append("")
    md.append("| Category | Avg Quality Score | Status |")
    md.append("|----------|------------------|--------|")

    for category, score in sorted(
        summary["category_scores"].items(), key=lambda x: x[1], reverse=True
    ):
        status = "✅" if score >= 70 else "⚠️" if score >= 50 else "❌"
        md.append(f"| {category.capitalize()} | {score:.1f}/100 | {status} |")

    md.append("")

    # Common Issues
    if summary["common_issues"]:
        md.append("## Common Issues")
        md.append("")
        md.append("Issues found across multiple functions:")
        md.append("")

        sorted_issues = sorted(summary["common_issues"].items(), key=lambda x: x[1], reverse=True)[
            :10
        ]

        for issue, count in sorted_issues:
            pct = (count / summary["total_functions"]) * 100
            md.append(f"- **{issue}**: {count} functions ({pct:.1f}%)")

        md.append("")

    # Detailed Function Comparisons
    md.append("## Detailed Function Comparisons")
    md.append("")

    # Group by category
    by_category: dict[str, list[Any]] = {}
    for result in results:
        cat = result["category"]
        if cat not in by_category:
            by_category[cat] = []
        by_category[cat].append(result)

    for category, cat_results in sorted(by_category.items()):
        md.append(f"### {category.capitalize()}")
        md.append("")

        for result in sorted(cat_results, key=lambda x: x["quality_score"], reverse=True):
            md.append(f"#### `{result['function_name']}`")
            md.append("")

            # Metrics table
            md.append("| Metric | Generated | Baseline | Ratio/Score |")
            md.append("|--------|-----------|----------|-------------|")
            md.append(f"| **Quality Score** | - | - | {result['quality_score']:.1f}/100 |")
            md.append(
                f"| **Structure Similarity** | - | - | {result['structure_similarity']:.1f}% |"
            )
            md.append(
                f"| **Lines of Code** | {result['generated_metrics']['lines_of_code']} | "
                f"{result['baseline_metrics']['lines_of_code']} | "
                f"{result['conciseness_ratio']:.2f}x |"
            )
            md.append(
                f"| **Complexity** | {result['generated_metrics']['complexity_score']} | "
                f"{result['baseline_metrics']['complexity_score']} | "
                f"{result['complexity_ratio']:.2f}x |"
            )
            md.append(
                f"| **Has TSDoc** | {'✅' if result['generated_metrics']['has_tsdoc'] else '❌'} | "
                f"{'✅' if result['baseline_metrics']['has_tsdoc'] else '❌'} | - |"
            )
            md.append(
                f"| **Type Annotations** | {'✅' if result['generated_metrics']['has_type_annotations'] else '❌'} | "
                f"{'✅' if result['baseline_metrics']['has_type_annotations'] else '❌'} | - |"
            )
            md.append(f"| **Generation Time** | {result['generation_time_ms']:.1f}ms | - | - |")
            md.append("")

            # Issues
            if result["issues"]:
                md.append("**Issues Found:**")
                for issue in result["issues"]:
                    md.append(f"- {issue}")
                md.append("")

            # Code comparison
            md.append("<details>")
            md.append("<summary>View Code Comparison</summary>")
            md.append("")
            md.append("**Generated Code:**")
            md.append("```typescript")
            md.append(result["generated_code"])
            md.append("```")
            md.append("")
            md.append("**Human Baseline:**")
            md.append("```typescript")
            md.append(result["baseline_code"])
            md.append("```")
            md.append("</details>")
            md.append("")

    # Recommendations
    md.append("## Recommendations for Improvement")
    md.append("")

    recommendations = []

    # Based on common issues
    if summary["common_issues"]:
        top_issues = sorted(summary["common_issues"].items(), key=lambda x: x[1], reverse=True)[:3]

        for issue, _count in top_issues:
            if "Missing TSDoc" in issue:
                recommendations.append(
                    "**Improve TSDoc generation**: Ensure all generated functions include "
                    "comprehensive TSDoc comments with `@param` and `@returns` tags."
                )
            elif "Missing type annotations" in issue:
                recommendations.append(
                    "**Strengthen type inference**: Improve type annotation coverage to match "
                    "human-written code standards."
                )
            elif "Complexity too high" in issue:
                recommendations.append(
                    "**Simplify generated logic**: Review generation patterns to produce more "
                    "concise, readable code with lower cyclomatic complexity."
                )
            elif "too verbose" in issue:
                recommendations.append(
                    "**Reduce verbosity**: Generate more concise code by using modern TypeScript "
                    "features (arrow functions, ternary operators, etc.)."
                )

    # Based on metrics
    if avg_similarity < 60:
        recommendations.append(
            "**Improve structural similarity**: Generated code structure differs significantly "
            "from human patterns. Review generation templates and prompts."
        )

    if avg_complexity > 1.5:
        recommendations.append(
            "**Reduce complexity**: Generated code is significantly more complex than baselines. "
            "Focus on simpler, more direct implementations."
        )

    if avg_concise > 1.5:
        recommendations.append(
            "**Increase conciseness**: Generated code uses more lines than necessary. "
            "Leverage TypeScript's expressive syntax (arrow functions, destructuring, etc.)."
        )

    # Category-specific recommendations
    for category, score in summary["category_scores"].items():
        if score < 60:
            recommendations.append(
                f"**Improve {category} generation**: This category has the lowest quality score "
                f"({score:.1f}/100). Review category-specific patterns."
            )

    if not recommendations:
        recommendations.append(
            "**Excellent quality!** Generated code meets or exceeds quality targets. "
            "Continue monitoring and maintain current standards."
        )

    for i, rec in enumerate(recommendations, 1):
        md.append(f"{i}. {rec}")
        md.append("")

    # Conclusion
    md.append("## Conclusion")
    md.append("")

    if avg_quality >= 70:
        md.append(
            f"The TypeScript generator achieves **high quality** output with an average score of "
            f"**{avg_quality:.1f}/100**. Generated code is competitive with human-written baselines "
            f"across most categories."
        )
    elif avg_quality >= 50:
        md.append(
            f"The TypeScript generator achieves **moderate quality** output with an average score of "
            f"**{avg_quality:.1f}/100**. There is room for improvement, particularly in "
            f"{'complexity' if avg_complexity > 1.5 else 'structure similarity'}."
        )
    else:
        md.append(
            f"The TypeScript generator currently produces **below-target quality** output with an "
            f"average score of **{avg_quality:.1f}/100**. Significant improvements needed across "
            f"multiple dimensions. Review recommendations above."
        )

    md.append("")
    md.append("---")
    md.append("")
    md.append(
        f"*Report generated automatically from benchmark data on {datetime.now().strftime('%Y-%m-%d')}*"
    )

    # Write to file
    with open(output_file, "w") as f:
        f.write("\n".join(md))

    print(f"\n✓ Markdown report generated: {output_file}")


def main():
    """CLI entry point for report generation."""
    parser = argparse.ArgumentParser(
        description="Generate markdown comparison report from benchmark results"
    )
    parser.add_argument(
        "results_file",
        type=Path,
        help="Path to benchmark JSON results file",
    )
    parser.add_argument(
        "-o",
        "--output",
        type=Path,
        help="Output markdown file (default: derived from results file)",
    )

    args = parser.parse_args()

    # Derive output filename if not provided
    if args.output:
        output_file = args.output
    else:
        output_file = args.results_file.parent / f"{args.results_file.stem}_report.md"

    # Generate report
    generate_markdown_report(args.results_file, output_file)


if __name__ == "__main__":
    main()
